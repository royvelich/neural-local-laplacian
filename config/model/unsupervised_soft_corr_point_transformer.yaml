_target_: neural_signatures.modules.unsupervised_modules.SoftCorrespondenceModule
pre_mlp:
  _target_: neural_signatures.modules.architectures.ConfigurableMLP
  channels: [14, 32]
  use_batch_norm: False
  activation: torch.nn.GELU
post_mlp:
  _target_: neural_signatures.modules.architectures.ConfigurableMLP
  channels: [480, 256, 128, 64]
  use_batch_norm: False
  activation: torch.nn.GELU
#  activation: neural_signatures.nn.activations.Sine
gnn:
  _target_: neural_signatures.modules.architectures.ConfigurableGNN
#  _recursive_: False
  conv_layers:
    - _target_: torch_geometric.nn.conv.PointTransformerConv
      in_channels: 32
      out_channels: 32
      pos_nn:
        _target_: neural_signatures.modules.architectures.ConfigurableMLP
        channels: [3, 32]
        use_batch_norm: False
        activation: torch.nn.GELU
      attn_nn:
        _target_: neural_signatures.modules.architectures.ConfigurableMLP
        channels: [32, 32]
        use_batch_norm: False
        activation: torch.nn.GELU
#      k: 20
#      nn:
#        _target_: neural_signatures.modules.architectures.ConfigurableMLP
#        channels: [64, 64, 64]
#        use_batch_norm: True
#        activation: torch.nn.GELU
    - _target_: torch_geometric.nn.conv.PointTransformerConv
      in_channels: 32
      out_channels: 64
      pos_nn:
        _target_: neural_signatures.modules.architectures.ConfigurableMLP
        channels: [3, 32, 64]
        use_batch_norm: False
        activation: torch.nn.GELU
      attn_nn:
        _target_: neural_signatures.modules.architectures.ConfigurableMLP
        channels: [64, 64]
        use_batch_norm: False
        activation: torch.nn.GELU
#      k: 20
#      nn:
#        _target_: neural_signatures.modules.architectures.ConfigurableMLP
#        channels: [128, 128, 128]
#        use_batch_norm: True
#        activation: torch.nn.GELU
    - _target_: torch_geometric.nn.conv.PointTransformerConv
      in_channels: 64
      out_channels: 128
      pos_nn:
        _target_: neural_signatures.modules.architectures.ConfigurableMLP
        channels: [3, 32, 128]
        use_batch_norm: False
        activation: torch.nn.GELU
      attn_nn:
        _target_: neural_signatures.modules.architectures.ConfigurableMLP
        channels: [128, 128]
        use_batch_norm: False
        activation: torch.nn.GELU
#      k: 20
#      nn:
#        _target_: neural_signatures.modules.architectures.ConfigurableMLP
#        channels: [256, 256, 256]
#        use_batch_norm: True
#        activation: torch.nn.GELU
    - _target_: torch_geometric.nn.conv.PointTransformerConv
      in_channels: 128
      out_channels: 256
      pos_nn:
        _target_: neural_signatures.modules.architectures.ConfigurableMLP
        channels: [3, 32, 256]
        use_batch_norm: False
        activation: torch.nn.GELU
      attn_nn:
        _target_: neural_signatures.modules.architectures.ConfigurableMLP
        channels: [256, 256]
        use_batch_norm: False
        activation: torch.nn.GELU
#      k: 20
#      nn:
#        _target_: neural_signatures.modules.architectures.ConfigurableMLP
#        channels: [512, 512, 512]
#        use_batch_norm: True
#        activation: torch.nn.GELU
#  conv_class_name: torch_geometric.nn.conv.PointTransformerConv
#  conv_args:
#    nn:
#  channels: [64, 64, 64, 128, 128]
#  k: null
  k: 20
  recompute_knn: True
#  pool_ratio: 0.5
  concat_residual: True
#  use_batch_norm: False
#  activation_name: null
#  activation_name: neural_signatures.nn.activations.Sine
sinkhorn_temp: 0.2
sinkhorn_n_iter: 10
sinkhorn_slack: False
pooling_layers: null
gnn_channels_key: 'channels'
gnn_channels_concat: True
